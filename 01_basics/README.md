# ğŸ“ Temel Kavramlar - VRP ve Reinforcement Learning'e GiriÅŸ

## ğŸ“– Ä°Ã§indekiler

1. [VRP Nedir?](#vrp-nedir)
2. [VRP Ortam YapÄ±sÄ±](#vrp-ortam-yapÄ±sÄ±)
3. [Reinforcement Learning Temelleri](#rl-temelleri)
4. [ModÃ¼l AÃ§Ä±klamalarÄ±](#modÃ¼l-aÃ§Ä±klamalarÄ±)

---

## ğŸš— VRP Nedir?

**Vehicle Routing Problem (VRP)**, kombinatoryal optimizasyon problemlerinden biridir. Temel olarak:

> Bir depodan baÅŸlayarak, belirli sayÄ±da mÃ¼ÅŸteriyi ziyaret edip tekrar depoya dÃ¶nen optimal rotayÄ± bulma problemi.

### VRP VaryantlarÄ±

1. **TSP (Traveling Salesman Problem)**
   - En basit form: tek araÃ§, tÃ¼m noktalarÄ± ziyaret et
   - VRP'nin temel hali

2. **CVRP (Capacitated VRP)**
   - Her mÃ¼ÅŸterinin talebi var
   - AraÃ§ kapasitesi sÄ±nÄ±rlÄ±
   - Birden fazla tur gerekebilir

3. **VRPTW (VRP with Time Windows)**
   - Her mÃ¼ÅŸterinin zaman penceresi var
   - Belirli saatler arasÄ±nda ziyaret edilmeli

4. **EVRP (Electric VRP)**
   - Elektrikli araÃ§lar
   - Åarj istasyonlarÄ±
   - Batarya kÄ±sÄ±tlarÄ±

### Neden Zor?

- **NP-Hard** problem: n mÃ¼ÅŸteri iÃ§in n! olasÄ± permÃ¼tasyon
- 20 mÃ¼ÅŸteri = 2.43 Ã— 10^18 olasÄ± tur!
- Exact Ã§Ã¶zÃ¼mler bÃ¼yÃ¼k problemlerde imkansÄ±z
- Metasezgiseller (ALNS, Tabu Search) kullanÄ±lÄ±r

### Deep Learning ile Ã‡Ã¶zÃ¼m

**Geleneksel YÃ¶ntemler:**
- Matematiksel optimizasyon (MIP)
- Metasezgiseller (ALNS, VNS, Tabu Search)
- Her problem iÃ§in yeniden Ã§alÄ±ÅŸtÄ±r

**Deep Learning AvantajlarÄ±:**
- âœ… Ã–ÄŸrenilmiÅŸ bir model: bir kez eÄŸit, hÄ±zlÄ±ca Ã§Ã¶z
- âœ… Generalization: farklÄ± boyutlarda Ã§alÄ±ÅŸabilir
- âœ… End-to-end: feature engineering yok
- âœ… Scalability: bÃ¼yÃ¼k problemlere adapt olabilir

**Dezavantajlar:**
- âŒ EÄŸitim sÃ¼resi uzun
- âŒ Optimal garanti yok
- âŒ KÄ±sÄ±tlÄ± problemlerde (VRPTW) daha zor

---

## ğŸ—ï¸ VRP Ortam YapÄ±sÄ±

### VRPInstance SÄ±nÄ±fÄ±

Problem tanÄ±mÄ±nÄ± tutar:

```python
instance = VRPInstance(
    depot=(0.5, 0.5),          # Depo koordinatlarÄ±
    customers=[(x1,y1), ...],   # MÃ¼ÅŸteri koordinatlarÄ±
    demands=[d1, d2, ...],      # Opsiyonel: mÃ¼ÅŸteri talepleri
    capacity=100                # Opsiyonel: araÃ§ kapasitesi
)
```

**Ã–zellikler:**
- `num_customers`: MÃ¼ÅŸteri sayÄ±sÄ±
- `to_tensor()`: PyTorch tensor'Ã¼ne Ã§evir
- `distance_matrix()`: Mesafe matrisini hesapla

### VRPEnvironment SÄ±nÄ±fÄ±

RL agent'Ä±nÄ±n etkileÅŸim kurduÄŸu ortam:

```python
env = VRPEnvironment(instance)
state = env.reset()                           # BaÅŸlangÄ±Ã§
action = agent.select_action(state)           # Aksiyon seÃ§
next_state, reward, done, info = env.step(action)  # AdÄ±m at
```

**State (Durum):**
```python
state = {
    'coords': Tensor[N, 2],      # Koordinatlar
    'visited': Tensor[N],         # Ziyaret mask (0/1)
    'current': Tensor[N]          # Mevcut lokasyon (one-hot)
}
```

**Action (Aksiyon):**
- Integer: 0 (depo), 1, 2, ..., N (mÃ¼ÅŸteriler)
- Gidilecek lokasyonun index'i

**Reward (Ã–dÃ¼l):**
- Negatif mesafe: `reward = -distance`
- Daha kÄ±sa mesafe = daha yÃ¼ksek Ã¶dÃ¼l
- GeÃ§ersiz aksiyon = bÃ¼yÃ¼k ceza (-1000)

**Done (Bitti mi):**
- `True`: TÃ¼m mÃ¼ÅŸteriler ziyaret edildi ve depoya dÃ¶nÃ¼ldÃ¼
- `False`: Episode devam ediyor

---

## ğŸ¤– RL Temelleri

### Markov Decision Process (MDP)

VRP bir MDP olarak modellenir:

**TanÄ±m:** MDP = (S, A, P, R, Î³)
- **S**: State space (durum uzayÄ±)
- **A**: Action space (aksiyon uzayÄ±)
- **P**: Transition probability (geÃ§iÅŸ olasÄ±lÄ±ÄŸÄ±) - VRP'de deterministik
- **R**: Reward function (Ã¶dÃ¼l fonksiyonu)
- **Î³**: Discount factor (indirim faktÃ¶rÃ¼)

### VRP MDP Representation

| BileÅŸen | VRP'de KarÅŸÄ±lÄ±ÄŸÄ± |
|---------|------------------|
| State | Koordinatlar + ziyaret durumu + mevcut lokasyon |
| Action | Bir sonraki ziyaret edilecek mÃ¼ÅŸteri |
| Reward | Negatif mesafe (kÄ±sa tur = yÃ¼ksek Ã¶dÃ¼l) |
| Done | TÃ¼m mÃ¼ÅŸteriler ziyaret edildi mi? |

### RL Ã–ÄŸrenme SÃ¼reci

```
1. Agent durumu gÃ¶zlemler (observe state)
2. Policy'ye gÃ¶re aksiyon seÃ§er (select action)
3. Ortam aksiyonu uygular (execute action)
4. Ã–dÃ¼l alÄ±r (receive reward)
5. Yeni duruma geÃ§er (transition to new state)
6. Deneyimden Ã¶ÄŸrenir (learn from experience)
```

### Exploration vs Exploitation

**Exploration:** Yeni aksiyonlar dene, bilinmeyeni keÅŸfet
**Exploitation:** Bilinen en iyi aksiyonu kullan

**Stochastic Policy:** Ï€(a|s) olasÄ±lÄ±ksal
- DoÄŸal exploration saÄŸlar
- Deterministic policy'den daha robust

---

## ğŸ“ ModÃ¼l AÃ§Ä±klamalarÄ±

### 1. `vrp_environment.py`

**Ana SÄ±nÄ±flar:**
- `VRPInstance`: Problem tanÄ±mÄ±
- `VRPEnvironment`: RL ortamÄ±
- `generate_random_vrp_instance()`: Rastgele problem Ã¼retici

**KullanÄ±m:**
```python
from vrp_environment import *

# Problem oluÅŸtur
instance = generate_random_vrp_instance(num_customers=10, seed=42)

# OrtamÄ± baÅŸlat
env = VRPEnvironment(instance)
state = env.reset()

# Greedy nearest neighbor ile Ã§Ã¶z
while not env.get_tour_info()['is_complete']:
    valid = env.get_valid_actions()
    # En yakÄ±n mÃ¼ÅŸteriyi seÃ§
    distances = [env.dist_matrix[env.current_location, a] for a in valid]
    action = valid[np.argmin(distances)]
    state, reward, done, info = env.step(action)

print(f"Tour: {env.tour}")
print(f"Total Distance: {env.total_distance:.3f}")
```

### 2. `visualizer.py`

**Fonksiyonlar:**
- `plot_vrp_tour()`: Tek tur gÃ¶rselleÅŸtir
- `plot_training_progress()`: EÄŸitim grafikleri
- `plot_multiple_tours()`: TurlarÄ± karÅŸÄ±laÅŸtÄ±r

**KullanÄ±m:**
```python
from visualizer import plot_vrp_tour

coords = [(0.5, 0.5), (0.2, 0.8), (0.7, 0.3)]
tour = [0, 1, 2, 0]

plot_vrp_tour(coords, tour, 
              title="My VRP Solution",
              save_path="my_tour.png",
              show=True)
```

---

## ğŸ§ª Ä°lk Deneyler

### 1. OrtamÄ± Test Et

```bash
cd 01_basics
python vrp_environment.py
```

**Beklenen Ã‡Ä±ktÄ±:**
- Problem detaylarÄ±
- Greedy NN ile oluÅŸturulmuÅŸ bir tur
- Toplam mesafe

### 2. GÃ¶rselleÅŸtirmeyi Test Et

```bash
python visualizer.py
```

**Beklenen Ã‡Ä±ktÄ±:**
- `experiments/results/` klasÃ¶rÃ¼nde grafikler
- Test turlarÄ± ve karÅŸÄ±laÅŸtÄ±rmalar

### 3. Kendi Probleminizi OluÅŸturun

```python
import numpy as np
from vrp_environment import VRPInstance, VRPEnvironment
from visualizer import plot_vrp_tour

# Manuel problem tanÄ±mla
instance = VRPInstance(
    depot=(50, 50),
    customers=[
        (20, 80), (80, 20), (30, 30), 
        (70, 70), (50, 90)
    ]
)

env = VRPEnvironment(instance)

# Random policy ile Ã§Ã¶z
state = env.reset()
import random
while not env.get_tour_info()['is_complete']:
    valid = env.get_valid_actions()
    action = random.choice(valid)
    state, reward, done, info = env.step(action)

# GÃ¶rselleÅŸtir
coords = [instance.depot] + instance.customers
plot_vrp_tour(coords, env.tour, title="Random Policy Tour")
```

---

## â“ SÄ±k Sorulan Sorular

**S: VRP'yi neden RL ile Ã§Ã¶zÃ¼yoruz?**
**C:** RL, sequential decision making problemleri iÃ§in idealdir. VRP'de her adÄ±mda "sonraki hangi mÃ¼ÅŸteri?" kararÄ± veriyoruz - bu bir RL problemidir.

**S: Neden state representation bu ÅŸekilde?**
**C:** Koordinatlar spatial bilgi, visited mask geÃ§miÅŸ bilgi, current mask mevcut durumu verir. Agent'Ä±n karar vermesi iÃ§in yeterli bilgi.

**S: Reward neden negatif mesafe?**
**C:** Daha kÄ±sa tur = daha iyi. Negatif mesafe kullanarak, maksimizasyon problemi haline getiriyoruz (RL standard'Ä±).

**S: Ortam deterministik mi?**
**C:** Evet, aynÄ± aksiyonu aynÄ± durumda uygularsanÄ±z aynÄ± sonucu alÄ±rsÄ±nÄ±z. Stochasticity policy'de (agent'ta).

---

## ğŸš€ Sonraki AdÄ±mlar

Temelleri Ã¶ÄŸrendiniz! Åimdi:

1. âœ… **Policy Gradient** (02_rl_methods/policy_gradient)
   - Basit RL ile VRP Ã§Ã¶zÃ¼mÃ¼
   - REINFORCE algoritmasÄ±

2. â­ï¸ **Actor-Critic** (02_rl_methods/actor_critic)
   - Daha kararlÄ± Ã¶ÄŸrenme
   - Value function entegrasyonu

3. â­ï¸ **Attention Mechanism** (02_rl_methods/attention_rl)
   - Modern VRP Ã§Ã¶zÃ¼mleri
   - State-of-the-art performans

---

**HazÄ±r mÄ±sÄ±nÄ±z?** Ä°lk RL modelinizi eÄŸitmeye baÅŸlayalÄ±m! ğŸ¯

```bash
cd ../02_rl_methods/policy_gradient
python simple_policy.py
```
