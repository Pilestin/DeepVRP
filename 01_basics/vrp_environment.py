"""
VRP Environment - Temel VRP Ortam TanÄ±mÄ±

Bu modÃ¼l, Vehicle Routing Problem (VRP) ve Traveling Salesman Problem (TSP) 
iÃ§in temel ortam yapÄ±sÄ±nÄ± saÄŸlar.

Autor: Yasin
Tarih: 28 Ekim 2025
"""

import numpy as np
import torch
from typing import Tuple, List, Optional
from dataclasses import dataclass


@dataclass
class VRPInstance:
    """
    Bir VRP problem Ã¶rneÄŸini temsil eder.
    
    Attributes:
        depot: Depo koordinatlarÄ± (x, y)
        customers: MÃ¼ÅŸteri koordinatlarÄ± liste [(x1, y1), (x2, y2), ...]
        demands: Her mÃ¼ÅŸterinin talep miktarÄ± (opsiyonel, CVRP iÃ§in)
        capacity: AraÃ§ kapasitesi (opsiyonel, CVRP iÃ§in)
        time_windows: Zaman pencereleri (opsiyonel, VRPTW iÃ§in)
    """
    depot: Tuple[float, float]
    customers: List[Tuple[float, float]]
    demands: Optional[List[float]] = None
    capacity: Optional[float] = None
    time_windows: Optional[List[Tuple[float, float]]] = None
    
    @property
    def num_customers(self) -> int:
        return len(self.customers)
    
    def to_tensor(self) -> torch.Tensor:
        """Problem Ã¶rneÄŸini PyTorch tensor'Ã¼ne Ã§evir"""
        all_coords = [self.depot] + self.customers
        return torch.FloatTensor(all_coords)
    
    def distance_matrix(self) -> np.ndarray:
        """TÃ¼m noktalar arasÄ± Ã–klid mesafe matrisini hesapla"""
        all_coords = np.array([self.depot] + self.customers)
        n = len(all_coords)
        dist_matrix = np.zeros((n, n))
        
        for i in range(n):
            for j in range(n):
                dist_matrix[i, j] = np.linalg.norm(all_coords[i] - all_coords[j])
        
        return dist_matrix


class VRPEnvironment:
    """
    VRP/TSP iÃ§in Reinforcement Learning ortamÄ±.
    
    Bu sÄ±nÄ±f, RL agent'Ä±nÄ±n etkileÅŸim kuracaÄŸÄ± ortamÄ± tanÄ±mlar.
    State (durum), action (aksiyon), reward (Ã¶dÃ¼l) kavramlarÄ±nÄ± iÃ§erir.
    """
    
    def __init__(self, instance: VRPInstance):
        """
        Args:
            instance: Ã‡Ã¶zÃ¼lecek VRP problem Ã¶rneÄŸi
        """
        self.instance = instance
        self.num_customers = instance.num_customers
        self.dist_matrix = instance.distance_matrix()
        
        # Ortam durumu
        self.current_location = 0  # BaÅŸlangÄ±Ã§: depo (index 0)
        self.visited = set([0])    # Ziyaret edilen lokasyonlar
        self.tour = [0]            # Tur sÄ±rasÄ±
        self.total_distance = 0.0  # Toplam mesafe
        
    def reset(self) -> torch.Tensor:
        """
        OrtamÄ± baÅŸlangÄ±Ã§ durumuna getir.
        
        Returns:
            Initial state (baÅŸlangÄ±Ã§ durumu)
        """
        self.current_location = 0
        self.visited = set([0])
        self.tour = [0]
        self.total_distance = 0.0
        
        return self._get_state()
    
    def _get_state(self) -> torch.Tensor:
        """
        Mevcut durumu tensor formatÄ±nda dÃ¶ndÃ¼r.
        
        State representation:
        - TÃ¼m lokasyon koordinatlarÄ±
        - Hangi lokasyonlarÄ±n ziyaret edildiÄŸi (binary mask)
        - Mevcut lokasyon
        """
        coords = self.instance.to_tensor()
        
        # Visited mask: 1 = ziyaret edildi, 0 = ziyaret edilmedi
        visited_mask = torch.zeros(len(coords))
        for idx in self.visited:
            visited_mask[idx] = 1
        
        # Current location one-hot encoding
        current_mask = torch.zeros(len(coords))
        current_mask[self.current_location] = 1
        
        # State'i birleÅŸtir
        state = {
            'coords': coords,
            'visited': visited_mask,
            'current': current_mask
        }
        
        return state
    
    def step(self, action: int) -> Tuple[torch.Tensor, float, bool, dict]:
        """
        Bir aksiyon uygula ve ortamÄ± gÃ¼ncelle.
        
        Args:
            action: Gidilecek lokasyon index'i (0: depo, 1-n: mÃ¼ÅŸteriler)
        
        Returns:
            next_state: Yeni durum
            reward: Ã–dÃ¼l (genelde negatif mesafe)
            done: Episode bitti mi?
            info: Ek bilgiler
        """
        # Aksiyonun geÃ§erli olup olmadÄ±ÄŸÄ±nÄ± kontrol et
        if action in self.visited and action != 0:
            # GeÃ§ersiz aksiyon: zaten ziyaret edilmiÅŸ (depo hariÃ§)
            reward = -1000.0  # BÃ¼yÃ¼k ceza
            done = False
            info = {'error': 'Invalid action: already visited'}
            return self._get_state(), reward, done, info
        
        # Mesafeyi hesapla
        distance = self.dist_matrix[self.current_location, action]
        
        # OrtamÄ± gÃ¼ncelle
        self.tour.append(action)
        self.visited.add(action)
        self.total_distance += distance
        self.current_location = action
        
        # Reward: negatif mesafe (daha kÄ±sa = daha iyi)
        reward = -distance
        
        # Episode bitti mi kontrol et
        # TÃ¼m mÃ¼ÅŸteriler ziyaret edildi ve depoya dÃ¶nÃ¼ldÃ¼ mÃ¼?
        done = (len(self.visited) == self.num_customers + 1 and action == 0)
        
        info = {
            'total_distance': self.total_distance,
            'tour_length': len(self.tour),
            'visited_count': len(self.visited)
        }
        
        return self._get_state(), reward, done, info
    
    def get_valid_actions(self) -> List[int]:
        """
        Mevcut durumda yapÄ±labilecek geÃ§erli aksiyonlarÄ± dÃ¶ndÃ¼r.
        
        Returns:
            GeÃ§erli aksiyon indeksleri listesi
        """
        valid = []
        
        # EÄŸer tÃ¼m mÃ¼ÅŸteriler ziyaret edildiyse, sadece depoya dÃ¶nÃ¼lebilir
        if len(self.visited) == self.num_customers + 1:
            return [0]
        
        # Ziyaret edilmemiÅŸ mÃ¼ÅŸteriler
        for i in range(1, self.num_customers + 1):
            if i not in self.visited:
                valid.append(i)
        
        return valid
    
    def get_tour_info(self) -> dict:
        """
        Mevcut turun detaylÄ± bilgisini dÃ¶ndÃ¼r.
        
        Returns:
            Tur bilgileri (sÄ±ra, mesafe, vb.)
        """
        return {
            'tour': self.tour.copy(),
            'total_distance': self.total_distance,
            'num_visited': len(self.visited),
            'is_complete': len(self.visited) == self.num_customers + 1
        }


def generate_random_vrp_instance(num_customers: int = 20, 
                                 grid_size: float = 1.0,
                                 seed: Optional[int] = None) -> VRPInstance:
    """
    Rastgele bir VRP/TSP problem Ã¶rneÄŸi Ã¼ret.
    
    Args:
        num_customers: MÃ¼ÅŸteri sayÄ±sÄ±
        grid_size: Koordinat alanÄ± boyutu [0, grid_size] x [0, grid_size]
        seed: Random seed (tekrarlanabilirlik iÃ§in)
    
    Returns:
        VRPInstance objesi
    """
    if seed is not None:
        np.random.seed(seed)
    
    # Depo: merkeze yakÄ±n rastgele bir nokta
    depot = (np.random.uniform(0.4, 0.6) * grid_size, 
             np.random.uniform(0.4, 0.6) * grid_size)
    
    # MÃ¼ÅŸteriler: rastgele daÄŸÄ±lmÄ±ÅŸ noktalar
    customers = [(np.random.uniform(0, grid_size), 
                  np.random.uniform(0, grid_size)) 
                 for _ in range(num_customers)]
    
    return VRPInstance(depot=depot, customers=customers)


if __name__ == "__main__":
    # Test kodu
    print("=" * 60)
    print("VRP Environment Test")
    print("=" * 60)
    
    # Rastgele bir problem Ã¶rneÄŸi oluÅŸtur
    instance = generate_random_vrp_instance(num_customers=5, seed=42)
    print(f"\nğŸ“¦ Problem: {instance.num_customers} mÃ¼ÅŸteri")
    print(f"   Depo: {instance.depot}")
    print(f"   Ä°lk 3 mÃ¼ÅŸteri: {instance.customers[:3]}")
    
    # OrtamÄ± baÅŸlat
    env = VRPEnvironment(instance)
    state = env.reset()
    print(f"\nğŸ¯ BaÅŸlangÄ±Ã§ durumu:")
    print(f"   Mevcut lokasyon: {env.current_location}")
    print(f"   GeÃ§erli aksiyonlar: {env.get_valid_actions()}")
    
    # Basit bir tur simÃ¼lasyonu (greedy nearest neighbor)
    print(f"\nğŸš— Greedy Nearest Neighbor Turu:")
    while not env.get_tour_info()['is_complete']:
        valid_actions = env.get_valid_actions()
        
        if len(valid_actions) == 1 and valid_actions[0] == 0:
            # Depoya dÃ¶n
            action = 0
        else:
            # En yakÄ±n mÃ¼ÅŸteriyi seÃ§
            distances = [env.dist_matrix[env.current_location, a] 
                        for a in valid_actions]
            action = valid_actions[np.argmin(distances)]
        
        state, reward, done, info = env.step(action)
        print(f"   AdÄ±m {len(env.tour)-1}: Lokasyon {action}, "
              f"Mesafe: {-reward:.3f}, Toplam: {info['total_distance']:.3f}")
        
        if done:
            break
    
    # Final sonuÃ§
    tour_info = env.get_tour_info()
    print(f"\nâœ… Tur tamamlandÄ±!")
    print(f"   Tur sÄ±rasÄ±: {tour_info['tour']}")
    print(f"   Toplam mesafe: {tour_info['total_distance']:.3f}")
    print("=" * 60)
