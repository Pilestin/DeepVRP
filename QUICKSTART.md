# ğŸš€ HÄ±zlÄ± BaÅŸlangÄ±Ã§ Rehberi

## ğŸ¯ HoÅŸ Geldiniz!

DeepVRP projenize hoÅŸ geldiniz! Bu rehber, projeyi kurmanÄ±z ve ilk derin Ã¶ÄŸrenme modelinizi eÄŸitmeniz iÃ§in adÄ±m adÄ±m yol gÃ¶sterecek.

## ğŸ“‹ Ã–nkoÅŸullar

âœ… Python 3.11.9 yÃ¼klÃ¼ (kontrol edildi)
âœ… Temel Python bilgisi
âœ… PyTorch Ã¶ÄŸrenme motivasyonu

## ğŸ”§ Kurulum AdÄ±mlarÄ±

### 1. Sanal Ortam OluÅŸturma

```powershell
# DeepVRP klasÃ¶rÃ¼ndeyken
python -m venv venv
```

### 2. Sanal OrtamÄ± AktifleÅŸtirme

```powershell
.\venv\Scripts\Activate
```

**Not:** EÄŸer execution policy hatasÄ± alÄ±rsanÄ±z:
```powershell
Set-ExecutionPolicy -ExecutionPolicy RemoteSigned -Scope CurrentUser
```

### 3. Gereksinimleri YÃ¼kleme

```powershell
pip install --upgrade pip
pip install torch torchvision numpy matplotlib networkx tqdm tensorboard pandas seaborn
```

**Alternatif:** requirements.txt'den yÃ¼kleyin:
```powershell
pip install -r requirements.txt
```

## ğŸ§ª Ä°lk Testler

### Test 1: VRP OrtamÄ±

```powershell
python 01_basics\vrp_environment.py
```

**Beklenen Ã‡Ä±ktÄ±:**
- Problem detaylarÄ±
- Greedy nearest neighbor Ã§Ã¶zÃ¼mÃ¼
- Toplam mesafe hesaplamasÄ±

### Test 2: GÃ¶rselleÅŸtirme

```powershell
python 01_basics\visualizer.py
```

**Beklenen Ã‡Ä±ktÄ±:**
- `experiments/results/` klasÃ¶rÃ¼nde grafikler oluÅŸturulmasÄ±

### Test 3: Utilities

```powershell
python utils\metrics.py
python utils\helpers.py
```

## ğŸ“ Ä°lk RL Modelini EÄŸitme

### Policy Gradient ile TSP Ã‡Ã¶zÃ¼mÃ¼

```powershell
python 02_rl_methods\policy_gradient\simple_policy.py
```

**Bu script:**
- 1000 episode boyunca bir REINFORCE agent'Ä± eÄŸitir
- 10 mÃ¼ÅŸterili TSP problemleri Ã§Ã¶zer
- Training progress grafiklerini kaydeder
- En iyi turu gÃ¶rselleÅŸtirir

**EÄŸitim sÃ¼resi:** ~5-10 dakika (CPU'da)

**SonuÃ§lar:**
- `experiments/models/policy_net_simple.pth` - EÄŸitilmiÅŸ model
- `experiments/results/training_progress.png` - EÄŸitim grafikleri
- `experiments/results/best_tour.png` - En iyi tur gÃ¶rselleÅŸtirmesi

## ğŸ“š Ã–ÄŸrenme Yol HaritasÄ±

### ğŸ”° Seviye 1: Temel Kavramlar (Hafta 1-2)

**OkumanÄ±z Gerekenler:**
1. `readme.md` - Genel proje yapÄ±sÄ±
2. `01_basics/README.md` - VRP ve RL temelleri
3. `02_rl_methods/policy_gradient/README.md` - Policy Gradient

**YapmanÄ±z Gerekenler:**
- [ ] VRP ortamÄ±nÄ± anlayÄ±n (`vrp_environment.py`)
- [ ] Kendi problem instance'larÄ±nÄ±zÄ± oluÅŸturun
- [ ] Basit heuristic'lerle (greedy, random) Ã§Ã¶zÃ¼mler Ã¼retin
- [ ] GÃ¶rselleÅŸtirmeleri inceleyin

**Pratik Egzersizler:**
```python
# Egzersiz 1: Kendi probleminizi oluÅŸturun
from basics.vrp_environment import VRPInstance, VRPEnvironment

instance = VRPInstance(
    depot=(50, 50),
    customers=[(20, 80), (80, 20), (30, 30), (70, 70)]
)

# Egzersiz 2: FarklÄ± Ã§Ã¶zÃ¼m stratejileri deneyin
# - Random policy
# - Nearest neighbor
# - Farthest insertion

# Egzersiz 3: SonuÃ§larÄ± karÅŸÄ±laÅŸtÄ±rÄ±n
```

### âš¡ Seviye 2: Policy Gradient (Hafta 3-4)

**OkumanÄ±z Gerekenler:**
- REINFORCE algoritmasÄ± teorisi
- Policy gradient matematiksel tÃ¼retme
- Variance reduction teknikleri

**YapmanÄ±z Gerekenler:**
- [ ] Simple policy modelini eÄŸitin
- [ ] Hiperparametre deneyimleri yapÄ±n
- [ ] FarklÄ± problem boyutlarÄ±nÄ± deneyin (5, 10, 20 mÃ¼ÅŸteri)
- [ ] Training curves'leri analiz edin

**Sorular:**
1. Policy gradient neden "gradient" terimi kullanÄ±yor?
2. Log probability neden kullanÄ±lÄ±yor?
3. Baseline variance'Ä± nasÄ±l azaltÄ±yor?
4. Exploration vs exploitation trade-off'u nedir?

### ğŸš€ Seviye 3: Ä°leri Seviye (Hafta 5+)

**Gelecek ModÃ¼ller:**
- Actor-Critic (daha stabil Ã¶ÄŸrenme)
- Attention Mechanism (modern VRP Ã§Ã¶zÃ¼mleri)
- Graph Neural Networks (graph representation)
- Pointer Networks (sequence-to-sequence)
- Transformer (state-of-the-art)

## ğŸ” Proje YapÄ±sÄ±

```
DeepVRP/
â”œâ”€â”€ ğŸ“„ readme.md              # Ana dokÃ¼mantasyon
â”œâ”€â”€ ğŸ“„ QUICKSTART.md          # Bu dosya
â”œâ”€â”€ ğŸ“„ requirements.txt       # Python paketleri
â”‚
â”œâ”€â”€ ğŸ“ 01_basics/             # â­ BURADAN BAÅLAYIN
â”‚   â”œâ”€â”€ vrp_environment.py    # VRP ortam tanÄ±mÄ±
â”‚   â”œâ”€â”€ visualizer.py         # GÃ¶rselleÅŸtirme
â”‚   â””â”€â”€ README.md             # DetaylÄ± aÃ§Ä±klamalar
â”‚
â”œâ”€â”€ ğŸ“ 02_rl_methods/         # â­ Ä°KÄ°NCÄ° ADIM
â”‚   â””â”€â”€ policy_gradient/      # Basit RL
â”‚       â”œâ”€â”€ simple_policy.py  # REINFORCE implementasyonu
â”‚       â””â”€â”€ README.md         # Teorik aÃ§Ä±klamalar
â”‚
â”œâ”€â”€ ğŸ“ 03_graph_methods/      # ğŸ”œ YAKINDA
â”‚   â”œâ”€â”€ gcn/                  # Graph Convolutional Network
â”‚   â”œâ”€â”€ gat/                  # Graph Attention Network
â”‚   â””â”€â”€ gnn_vrp/              # GNN ile VRP
â”‚
â”œâ”€â”€ ğŸ“ 04_sequence_methods/   # ğŸ”œ YAKINDA
â”‚   â”œâ”€â”€ seq2seq/              # Seq2Seq + Attention
â”‚   â”œâ”€â”€ pointer_network/      # Pointer Network
â”‚   â””â”€â”€ transformer/          # Transformer
â”‚
â”œâ”€â”€ ğŸ“ utils/                 # YardÄ±mcÄ± fonksiyonlar
â”‚   â”œâ”€â”€ metrics.py            # DeÄŸerlendirme metrikleri
â”‚   â””â”€â”€ helpers.py            # Genel yardÄ±mcÄ±lar
â”‚
â””â”€â”€ ğŸ“ experiments/           # SonuÃ§lar ve modeller
    â”œâ”€â”€ logs/                 # Training loglarÄ±
    â”œâ”€â”€ models/               # Kaydedilen modeller
    â””â”€â”€ results/              # Grafikler ve tablolar
```

## ğŸ’¡ Ä°puÃ§larÄ± ve PÃ¼f NoktalarÄ±

### Debugging

```python
# VRP ortamÄ±nÄ± adÄ±m adÄ±m inceleyin
env = VRPEnvironment(instance)
state = env.reset()

print("Initial state:", state)
print("Valid actions:", env.get_valid_actions())

# Her adÄ±mÄ± izleyin
for i in range(5):
    action = env.get_valid_actions()[0]
    state, reward, done, info = env.step(action)
    print(f"Step {i}: action={action}, reward={reward:.3f}, done={done}")
```

### Performans Ä°yileÅŸtirme

1. **GPU kullanÄ±n** (varsa):
```python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)
```

2. **Batch processing**:
- Birden fazla problem instance'Ä±nÄ± paralel Ã§Ã¶zÃ¼n
- EÄŸitim sÃ¼resini kÄ±saltÄ±n

3. **Checkpoint kaydetme**:
```python
from utils.helpers import save_checkpoint

save_checkpoint(model, optimizer, epoch, 
                'experiments/models/checkpoint.pth')
```

### SonuÃ§larÄ± Analiz Etme

```python
from utils.metrics import evaluate_multiple_solutions

# Birden fazla Ã§alÄ±ÅŸtÄ±rmanÄ±n istatistiklerini alÄ±n
stats = evaluate_multiple_solutions(coords, tours)
print(f"Mean: {stats['mean']:.3f}")
print(f"Std: {stats['std']:.3f}")
print(f"Best: {stats['min']:.3f}")
```

## ğŸ› SÄ±k KarÅŸÄ±laÅŸÄ±lan Sorunlar

### Problem: ModuleNotFoundError

**Ã‡Ã¶zÃ¼m:**
```powershell
# Sanal ortamÄ±n aktif olduÄŸundan emin olun
.\venv\Scripts\Activate

# Paketleri yeniden yÃ¼kleyin
pip install -r requirements.txt
```

### Problem: "Import Error" alÄ±yorum

**Ã‡Ã¶zÃ¼m:**
```python
# Script'leri proje kÃ¶k dizininden Ã§alÄ±ÅŸtÄ±rÄ±n
cd C:\Users\Yasin\Desktop\Kodlar\DeepVRP
python 01_basics\vrp_environment.py
```

### Problem: EÄŸitim Ã§ok yavaÅŸ

**Ã‡Ã¶zÃ¼m:**
- Episode sayÄ±sÄ±nÄ± azaltÄ±n (ilk testler iÃ§in 100-200)
- Daha kÃ¼Ã§Ã¼k problem boyutu kullanÄ±n (5-10 mÃ¼ÅŸteri)
- GPU kullanÄ±n (varsa)

### Problem: Model Ã¶ÄŸrenmiyor

**Kontrol Edin:**
- Learning rate'i ayarlayÄ±n (1e-4 veya 1e-2 deneyin)
- Network boyutunu deÄŸiÅŸtirin (hidden_dim)
- Reward function'Ä±n doÄŸru Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun

## ğŸ“ YardÄ±m ve Kaynaklar

### Ã–nerilen Okumalar

1. **Kitaplar:**
   - "Reinforcement Learning: An Introduction" - Sutton & Barto
   - "Deep Learning" - Goodfellow, Bengio, Courville

2. **Online Kaynaklar:**
   - OpenAI Spinning Up (RL tutorial)
   - PyTorch Tutorials
   - Papers with Code (VRP baÅŸlÄ±ÄŸÄ±)

3. **Ã–nemli Makaleler:**
   - "Attention, Learn to Solve Routing Problems!" (Kool et al., 2019)
   - "Pointer Networks" (Vinyals et al., 2015)
   - "Neural Combinatorial Optimization" (Bello et al., 2016)

### Deney Fikirleri

1. **FarklÄ± problem boyutlarÄ±**:
   - 5, 10, 20, 50, 100 mÃ¼ÅŸteri
   - Generalization yeteneÄŸini test edin

2. **Transfer learning**:
   - KÃ¼Ã§Ã¼k problemde eÄŸitin
   - BÃ¼yÃ¼k problemde test edin

3. **Hibrit yaklaÅŸÄ±mlar**:
   - RL + local search
   - RL ile initial solution, ALNS ile improve

## ğŸ¯ Sonraki AdÄ±mlar

Åimdi ÅŸunlarÄ± yapmalÄ±sÄ±nÄ±z:

1. âœ… Kurulumu tamamlayÄ±n
2. âœ… Temel testleri Ã§alÄ±ÅŸtÄ±rÄ±n
3. âœ… Ä°lk RL modelinizi eÄŸitin
4. ğŸ“– README dosyalarÄ±nÄ± okuyun
5. ğŸ’» Kod Ã¼zerinde denemeler yapÄ±n
6. ğŸ“Š SonuÃ§larÄ±nÄ±zÄ± analiz edin
7. ğŸš€ Ä°leri seviye modÃ¼llere geÃ§in

---

**BaÅŸarÄ±lar! ğŸ“**

SorularÄ±nÄ±z iÃ§in kod Ã¼zerinde notlar alÄ±n ve kendi deneyimlerinizi dokÃ¼mante edin. Bu proje sizin araÅŸtÄ±rma portfÃ¶yÃ¼nÃ¼zÃ¼n temelini oluÅŸturacak.
