# ğŸ¯ Policy Gradient ile VRP Ã‡Ã¶zÃ¼mÃ¼

## ğŸ“š Teorik Arka Plan

### Reinforcement Learning Temelleri

**Reinforcement Learning (RL)**, bir agent'Ä±n bir ortamda aksiyonlar alarak maksimum Ã¶dÃ¼l elde etmeyi Ã¶ÄŸrendiÄŸi bir makine Ã¶ÄŸrenmesi yaklaÅŸÄ±mÄ±dÄ±r.

#### Temel BileÅŸenler:

1. **State (Durum - s)**: OrtamÄ±n mevcut durumu
   - VRP'de: Hangi mÃ¼ÅŸterilerin ziyaret edildiÄŸi, mevcut konum, vb.

2. **Action (Aksiyon - a)**: Agent'Ä±n alabileceÄŸi eylemler
   - VRP'de: Bir sonraki ziyaret edilecek mÃ¼ÅŸteri

3. **Reward (Ã–dÃ¼l - r)**: Bir aksiyonun ne kadar iyi olduÄŸunu gÃ¶steren sinyal
   - VRP'de: Negatif mesafe (daha kÄ±sa = daha iyi)

4. **Policy (Politika - Ï€)**: Durumdan aksiyona mapping
   - Ï€(a|s): s durumunda a aksiyonunu alma olasÄ±lÄ±ÄŸÄ±

5. **Value Function (DeÄŸer Fonksiyonu - V)**: Bir durumun ne kadar iyi olduÄŸu
   - V(s): s durumundan baÅŸlayarak elde edilebilecek beklenen toplam Ã¶dÃ¼l

### Policy Gradient YÃ¶ntemleri

Policy Gradient, policy'yi **doÄŸrudan** parametreleÅŸtirip optimize eder.

**AmaÃ§:** Policy parametrelerini (Î¸) optimize et
```
J(Î¸) = E[âˆ‘t Î³^t * r_t]  (Beklenen toplam Ã¶dÃ¼l)
```

**Gradient:**
```
âˆ‡Î¸ J(Î¸) = E[âˆ‘t âˆ‡Î¸ log Ï€_Î¸(a_t|s_t) * G_t]
```

Burada:
- `G_t` = âˆ‘k Î³^k * r_(t+k) (t anÄ±ndan sonraki toplam Ã¶dÃ¼l)
- `Î³` = discount factor (gelecek Ã¶dÃ¼llerinin Ã¶nemi)

### REINFORCE AlgoritmasÄ±

**REINFORCE**, en basit policy gradient algoritmasÄ±dÄ±r:

1. Policy network ile bir episode oynat
2. Her adÄ±m iÃ§in log probability kaydet: `log Ï€(a_t|s_t)`
3. Episode sonunda returns hesapla: `G_t`
4. Loss hesapla: `L = -âˆ‘t log Ï€(a_t|s_t) * G_t`
5. Backpropagation ile gradient descent yap

**Avantajlar:**
- âœ… Basit ve anlaÅŸÄ±lÄ±r
- âœ… SÃ¼rekli aksiyon uzaylarÄ±nda Ã§alÄ±ÅŸÄ±r
- âœ… Stochastic policy Ã¶ÄŸrenir (exploration doÄŸal)

**Dezavantajlar:**
- âŒ YÃ¼ksek variance (Ã§Ã¶zÃ¼m: baseline kullan)
- âŒ Sample inefficient (Ã§ok deneyim gerekir)
- âŒ YavaÅŸ Ã¶ÄŸrenme

## ğŸ—ï¸ Mimari

### SimplePolicyNetwork

```
Input: 
  - Coordinates [N, 2]
  - Visited mask [N]
  - Current location [N]

â†“

Location Encoder (MLP):
  - Linear(2, 128)
  - ReLU
  - Linear(128, 128)

â†“

Context Encoder (MLP):
  - Linear(2N, 128)
  - ReLU
  - Linear(128, 128)

â†“

Policy Head:
  - Concat(location_feature, context_feature)
  - Linear(256, 128)
  - ReLU
  - Linear(128, N)

â†“

Output: Action probabilities [N]
```

### Neden Bu Mimari?

1. **Location Encoder**: Her lokasyonun spatial Ã¶zelliklerini Ã¶ÄŸrenir
2. **Context Encoder**: Global durum bilgisini (visited, current) encode eder
3. **Policy Head**: Ä°kisini birleÅŸtirip aksiyon olasÄ±lÄ±klarÄ±nÄ± Ã¼retir

## ğŸ’» KullanÄ±m

### Temel EÄŸitim

```python
from simple_policy import train_vrp_solver

# 10 mÃ¼ÅŸterili VRP iÃ§in 1000 episode eÄŸit
agent, rewards, distances = train_vrp_solver(
    num_episodes=1000,
    num_customers=10,
    print_every=100
)
```

### Kendi Probleminizi Ã‡Ã¶zme

```python
from simple_policy import REINFORCEAgent
from vrp_environment import VRPInstance, VRPEnvironment

# Problem tanÄ±mla
instance = VRPInstance(
    depot=(0.5, 0.5),
    customers=[(0.2, 0.3), (0.8, 0.7), (0.4, 0.9)]
)

# Agent oluÅŸtur ve eÄŸit
agent = REINFORCEAgent(num_locations=4)  # 3 mÃ¼ÅŸteri + 1 depo
env = VRPEnvironment(instance)

# Bir Ã§Ã¶zÃ¼m Ã¼ret
state = env.reset()
done = False
while not done:
    valid_actions = env.get_valid_actions()
    action = agent.select_action(state, valid_actions)
    state, reward, done, info = env.step(action)

tour_info = env.get_tour_info()
print(f"Tour: {tour_info['tour']}")
print(f"Distance: {tour_info['total_distance']}")
```

## ğŸ“Š Beklenen SonuÃ§lar

10 mÃ¼ÅŸterili TSP iÃ§in:
- **Ä°lk 100 episode**: Random turlar, mesafe ~7-10
- **200-500 episode**: Ã–ÄŸrenme baÅŸlar, mesafe ~5-7
- **500-1000 episode**: Ä°yi Ã§Ã¶zÃ¼mler, mesafe ~4-5
- **Optimal Ã§Ã¶zÃ¼m**: ~3.5-4.5 (problem instance'a baÄŸlÄ±)

**Not:** Bu basit model, attention mekanizmasÄ± olmadan sÄ±nÄ±rlÄ± performans gÃ¶sterir. Ä°lerleyen modÃ¼llerde Ã§ok daha iyi sonuÃ§lar alacaÄŸÄ±z!

## ğŸ”¬ Deneyler ve Ä°yileÅŸtirmeler

### Hiperparametre AyarlarÄ±

```python
# Learning rate
lr = 1e-3  # VarsayÄ±lan, 1e-4 veya 1e-2 deneyin

# Network boyutu
hidden_dim = 128  # 64, 256 deneyin

# Discount factor
gamma = 0.99  # 0.95, 1.0 deneyin

# Episode sayÄ±sÄ±
num_episodes = 1000  # Daha bÃ¼yÃ¼k problemler iÃ§in artÄ±rÄ±n
```

### Variance Azaltma

1. **Baseline ekle**: `G_t - V(s_t)` kullan (Actor-Critic'e geÃ§iÅŸ)
2. **Returns normalize et**: Mean/std ile normalize
3. **Advantage function**: `A(s,a) = Q(s,a) - V(s)`

### Ä°leri Seviye

- [ ] Baseline ekle (value network)
- [ ] Entropy regularization (exploration iÃ§in)
- [ ] Multiple workers (paralel Ã¶rnekleme)
- [ ] Attention mechanism ekle

## ğŸ“– Ã–ÄŸrenme SorularÄ±

1. **Policy Gradient neden "gradient" diyor?**
   - Cevap: Policy parametrelerinin gradient'ini hesaplayÄ±p, Ã¶dÃ¼lÃ¼ artÄ±racak yÃ¶nde gÃ¼ncelliyoruz.

2. **Neden log probability kullanÄ±yoruz?**
   - Cevap: Matematiksel olarak tÃ¼rev almayÄ± kolaylaÅŸtÄ±rÄ±r ve numerically stable.

3. **Variance problemi nedir?**
   - Cevap: Her episode farklÄ± returns verir, bu da gradient'lerde bÃ¼yÃ¼k deÄŸiÅŸiklikler yaratÄ±r.

4. **Bu yÃ¶ntem neden diÄŸer ML yÃ¶ntemlerinden farklÄ±?**
   - Cevap: Supervised learning gibi doÄŸru cevaplarÄ± yok, agent kendi deneyimlerinden Ã¶ÄŸreniyor.

## ğŸ”— Ä°leri Okuma

- **Sutton & Barto**: "Reinforcement Learning: An Introduction" (Kitap)
- **Karpathy Blog**: "Deep Reinforcement Learning: Pong from Pixels"
- **OpenAI Spinning Up**: RL dokÃ¼mantasyonu
- **Williams 1992**: "Simple Statistical Gradient-Following Algorithms..." (REINFORCE paper)

---

**Sonraki AdÄ±m:** Actor-Critic mimarisine geÃ§elim! ğŸš€
